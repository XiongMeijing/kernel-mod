{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to `kmod`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will introduce you to  `kmod`, a Python package implementing linear-time kernel-based model comparison tests as described in \n",
    "\n",
    "    Informative Features for Model Comparison\n",
    "    Wittawat Jitkrittum, Heishiro Kanagawa, Patsorn Sangkloy, James Hays, Bernhard SchÃ¶lkopf, Arthur Gretton\n",
    "    NIPS 2018\n",
    "    https://arxiv.org/abs/1810.11630\n",
    "\n",
    "See the [Github page](https://github.com/wittawatj/kernel-mod) for installation instructions. This package depends on\n",
    "\n",
    "* `kgof`: https://github.com/wittawatj/kernel-gof\n",
    "* `freqopttest`: https://github.com/wittawatj/interpretable-test\n",
    "\n",
    "Make sure that you have `kmod` included in Python's search path. In particular the following import statements should not produce any fatal error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#%config InlineBackend.figure_format = 'pdf'\n",
    "\n",
    "import kmod\n",
    "import kgof\n",
    "import kgof.goftest as gof\n",
    "# submodules\n",
    "from kmod import data, density, kernel, util, plot\n",
    "from kmod import mctest as mct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plotting options. This block of code can be removed.\n",
    "font = {\n",
    "    #'family' : 'normal',\n",
    "    #'weight' : 'bold',\n",
    "    'size'   : 16\n",
    "}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "plt.rc('lines', linewidth=2)\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [`autograd`](https://github.com/HIPS/autograd) to compute derivatives for our optimization problem. So instead of \n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "make sure you use \n",
    "\n",
    "    import autograd.numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative goodness-of-fit test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two candidate models $P$ and $Q$, and a sample $\\{z_i \\}_{i=1}^n \\sim R$, where $R$ is an unknown data generating distribution, we address the problem of determining the relative goodness of fit of the two models. The problem is formulated as a test proposing\n",
    "\n",
    "$H_0: D(P, R) \\le D(Q,R)$\n",
    "\n",
    "against \n",
    "\n",
    "$H_1: D(P, R) > D(Q,R),$\n",
    "\n",
    "where $D$ is a distance between two distributions. In words, $H_0$ states that $P$ is closer to $R$ than $Q$ is i.e., $P$ fits $R$ better. The alternative $H_1$ states the opposite i.e., $Q$ fits $R$ better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, we propose two related tests: Rel-UME and Rel-FSSD:\n",
    "\n",
    "* Rel-UME assumes that the two models $P$ and $Q$ are represented by two samples $\\{x_i\\}_{i=1}^n \\stackrel{i.i.d.}{\\sim} P$ and $\\{y_i\\}_{i=1}^n \\stackrel{i.i.d.}{\\sim} Q$, respectively.\n",
    "\n",
    "* Rel-FSSD assumes that the unnormalized probability density functions $p,q$ of the two models $P$ and $Q$ are available. \"Unnormalized\" here means that the normalizer of each model is not needed by the test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rel-UME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let us consider a two-dimensional example where $P,Q,R$ are mixtures of Gaussians. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p, q, r are mixtures of two Gaussians.\n",
    "shared_component = [6, 0]\n",
    "p_means = np.array([[-5, 0], shared_component])\n",
    "p = density.IsoGaussianMixture(p_means, variances=[1, 0.4], pmix=[0.7, 0.3])\n",
    "\n",
    "q_means=  np.array([[0, 1.3], shared_component])\n",
    "q = density.IsoGaussianMixture(q_means, variances=[1, 0.4], pmix=[0.7, 0.3])\n",
    "\n",
    "r_means = np.array([[0, 0], shared_component])\n",
    "r = density.IsoGaussianMixture(r_means, variances=[1, 0.4], pmix=[0.7, 0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, the three samples `X, Y, Z` are given. Here, for illustration, we will generate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P, Q, R must all be distinct. \n",
    "# Otherwise, it violates our assumption and the asymptotic null distribution does not hold.\n",
    "\n",
    "# sample size\n",
    "n = 400 \n",
    "seed = 10\n",
    "\n",
    "# Draw three samples\n",
    "datap = p.get_datasource().sample(n, seed=seed)\n",
    "dataq = q.get_datasource().sample(n, seed=seed+1)    \n",
    "datar = r.get_datasource().sample(n, seed=seed+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our code, a dataset is represented by an object of type `kmod.data.Data`. The input to the constructor of `kmod.data.Data` is a numpy array of size $n\\times d$ where $n$ is the sample size, and $d$ is the dimension. Given a dataset `X`, `kmod.data.Data` can be constructed as\n",
    "\n",
    "    datp = kmod.data.Data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d_data(X, Y, Z, V=None):\n",
    "    \"\"\"\n",
    "    * X, Y, Z: n x d numpy arrays representing the data\n",
    "    * V: J x d numpy arrays of J test locations\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    alpha = 0.5\n",
    "    plt.plot(X[:, 0], X[:, 1], 'ro', alpha=alpha, label='P')\n",
    "    plt.plot(Y[:, 0], Y[:, 1], 'bo', alpha=alpha, label='Q')\n",
    "    plt.plot(Z[:, 0], Z[:, 1], 'ko', alpha=alpha, label='R')\n",
    "    if V is not None:\n",
    "        for (i, v) in enumerate(V):\n",
    "            plt.plot(v[0], v[1], '^', color='m', markersize=20, label='V' if i==0 else '')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.title('H1: Q is closer to R')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datap.data()\n",
    "Y = dataq.data()\n",
    "Z = datar.data()\n",
    "\n",
    "# plot the data\n",
    "plot_2d_data(X, Y, Z)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the true distribution $R$ is a bimodal distribution (black). Both candidate models $P$ (red) and $Q$ (blue) are also bimodal distributions. Both models can capture well the right mode of $R$. However, $Q$ appears to capture the left mode of $R$ better (around the origin)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify $J$ test locations for testing\n",
    "\n",
    "Rel-UME measures relative goodness of fit of $P$ and $Q$ in the areas specified by a set of points. This set of points is known as the set of *test locations*, denoted by $V$. For simplicity, we will consider $J=1$ test location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the hyperparameters of the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandwidth of the Gaussian kernel. \n",
    "gw0 = 2**2\n",
    "\n",
    "# Gaussian kernel\n",
    "k0 = kernel.KGauss(gw0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration, let us specify the test location (shown as purple in the following plot) at the right mode of $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the test location(s)\n",
    "# V0 in general is a J x d numpy array of J test locations in d dimensions.\n",
    "V0 = np.array([[7, 0]])\n",
    "plot_2d_data(X, Y, Z, V0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the Rel-UME test. This is implemented in `kmod.mctest.SC_GaussUME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance level of the test\n",
    "alpha = 0.01  \n",
    "\n",
    "# Construct an object to represent the test\n",
    "scume0 = mct.SC_UME(datap, dataq, k0, k0, V0, V0, alpha=alpha)\n",
    "\n",
    "# Do the test. Returns a dictionary\n",
    "test_result = scume0.perform_test(datar)\n",
    "display(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the test fails to reject H0. This means that there is not enough evidence to reject the null hypothesis that $P$ is better, *as measured at V*. Indeed, at the right mode of $R$, both models $P$ and $Q$ are equally good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us consider placing the test location around the origin where there is a clear difference between $P$ and $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new test location\n",
    "V1 = np.array([[-1, 1]])\n",
    "plot_2d_data(X, Y, Z, V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the test with this new test location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scume1 = mct.SC_UME(datap, dataq, k0, k0, V1, V1, alpha=alpha)\n",
    "scume1.perform_test(datar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test rejects H0 with a small p-value in this case. This means that the test detects a significant difference in \n",
    "the (relative) goodness of fit of $P$ and $Q$, *as measured at V*. We have seen that the test result depends on the test location(s) V. This behaviour is a feature since it allows practitioners to pose a specific question: \"Is Q better than P as measured around V?\" In practice when $P$ and $Q$ are highly complicated, it is unlikely that one model is better than another uniformly over the input domain. That is, $P$ might better than $Q$ at $V_1$, and at the same time $Q$ might be better at $V_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the test locations and the Gaussian bandwidth\n",
    "\n",
    "The test locations $V$ and the Gaussian bandwidth can also be tuned automatically by  maximizing the test power criterion of the Rel-UME test. The optimized test locations indicate the regions in the input domain where the better fit of $Q$ can be detected with highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition the data into training and test sets. We will use the training set for hyperparameter optimization,  and use the test set for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training/test sets\n",
    "\n",
    "# proportion of the training data\n",
    "tr_proportion = 0.2\n",
    "[(datptr, datpte), (datqtr, datqte), (datrtr, datrte)] = \\\n",
    "    [D.split_tr_te(tr_proportion=tr_proportion, seed=85) for D in [datap, dataq, datar]]\n",
    "Xtr, Ytr, Ztr = [D.data() for D in [datptr, datqtr, datrtr]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_bandwidth(X, Y, Z):\n",
    "    \"\"\"\n",
    "    A median heuristic to select the bandwidth of the Gaussian kernel.\n",
    "    There are other variations.\n",
    "    \"\"\"\n",
    "    medxz = util.meddistance(np.vstack((X, Z)), subsample=1000)\n",
    "    medyz = util.meddistance(np.vstack((Y, Z)), subsample=1000)\n",
    "    gwidth0 = np.mean([medxz, medyz])**2\n",
    "    return gwidth0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize optimization parameters.\n",
    "# Initialize the Gaussian widths with the median heuristic\n",
    "gwidth0 = get_median_bandwidth(Xtr, Ytr, Ztr)\n",
    "\n",
    "# the number of test locations to optimize\n",
    "J = 1\n",
    "\n",
    "# pick a subset of points in the training set to initialize V\n",
    "Xyztr = np.vstack((Xtr, Ytr, Ztr))\n",
    "V0 = util.subsample_rows(Xyztr, J, seed=seed)\n",
    "\n",
    "# optimization options\n",
    "opt_options = {\n",
    "    'max_iter': 100, # maximum number of gradient ascent iterations\n",
    "    'reg': 1e-4, # regularization parameter in the optimization objective\n",
    "    'tol_fun': 1e-7, # termination tolerance of the objective\n",
    "    \n",
    "    #Define a box bounds for the testing locations. \n",
    "    #extend the box defined by coordinate-wise min-max by std of each \n",
    "    #coordinate (of the aggregated data) multiplied by this number.\n",
    "    'locs_bounds_frac': 50, \n",
    "\n",
    "    'gwidth_lb': 0.1, # absolute lower bound of the Gaussian width^2\n",
    "    'gwidth_ub': 5**2, # abolute upper bound of the Gaussian width^2\n",
    "}\n",
    "\n",
    "# Optimize on the training set\n",
    "V_opt, gw2_opt, opt_result = mct.SC_GaussUME.optimize_3sample_criterion(\n",
    "    datptr, datqtr, datrtr, V0, gwidth0, **opt_options)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization procedure returns back \n",
    "\n",
    "1. `V_opt`: optimized test locations (features). A $J \\times d$ numpy array.\n",
    "2. `gw2_opt`: optimized squared Gaussian width (for the Gaussian kernel). A floating point number.\n",
    "3. `opt_result`: a dictionary containing information gathered during the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Optimization details:')\n",
    "display(opt_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use these optimized parameters to construct a Rel-UME test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance level of the test\n",
    "alpha = 0.01  \n",
    "\n",
    "# Gaussian kernel constructed with the optimized squared bandwidth\n",
    "k_opt = kernel.KGauss(gw2_opt)\n",
    "\n",
    "# Construct an object to represent the test. Use the test data here (not the training data!)\n",
    "scume_opt3 = mct.SC_UME(datpte, datqte, k_opt, k_opt, V_opt, V_opt, alpha=alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform model comparison test on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary\n",
    "test_result = scume_opt3.perform_test(datrte)\n",
    "display(test_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data and the optimized test location(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_data(Xtr, Ytr, Ztr, V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rel-FSSD\n",
    "\n",
    "Rel-FSSD assumes that the unnormalized probability density functions $p,q$ of the two models $P$ and $Q$ are available. \"Unnormalized\" here means that the normalizer of each model is not needed by the test. The true data generating distribution $R$ is still represented as a sample $\\{ z_i \\}_{i=1}^n \\sim R$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let us consider the same toy example as in Figure 1c of our NIPS 2018 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models. p and q are Gaussian.\n",
    "p = density.IsotropicNormal(mean=np.array([0]), variance=1)\n",
    "q = density.IsotropicNormal(mean=np.array([6]), variance=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you can implement your own density functions easily. See https://github.com/wittawatj/kernel-gof/blob/master/ipynb/demo_kgof.ipynb . Note that the modules `kgof.density` and `kmod.density` are interchangeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r is a mixture of two Gaussians\n",
    "r = density.IsoGaussianMixture(\n",
    "    means=np.array([[0, 6]]).T,\n",
    "    variances=np.array([1, 1]),\n",
    "    pmix=[0.5, 0.5]\n",
    ")\n",
    "dsr = r.get_datasource()\n",
    "\n",
    "# datr is of type `kmod.data.Data`. \n",
    "# As stated earlier, this can be constructed from a numpy array as well.\n",
    "datr = dsr.sample(n=500, seed=59)\n",
    "\n",
    "# datr.data() gives the underlying numpy array\n",
    "Z = datr.data()\n",
    "\n",
    "# median heuristic\n",
    "med = util.meddistance(Z, subsample=1000)\n",
    "k = kernel.KGauss(sigma2=med**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_densities(p, q, Z, V=None):\n",
    "    # plot the densities\n",
    "    dom = np.linspace(-5, 13, 200)[:, np.newaxis]\n",
    "    denp = np.exp(p.log_normalized_den(dom))\n",
    "    denq = np.exp(q.log_normalized_den(dom))\n",
    "#     denr = np.exp(r.log_normalized_den(dom))\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.box(False)\n",
    "    density_lw = 2\n",
    "    plt.plot(dom, denp, 'r-', linewidth=density_lw, label='p')\n",
    "    plt.plot(dom, denq, 'b-', linewidth=density_lw, label='q')\n",
    "#     plt.plot(dom, denr, 'k-', linewidth=density_lw, label='r')\n",
    "    plt.hist(Z, density=True, color='k', alpha=0.6, label='$Z \\sim r$')\n",
    "    if V is not None:\n",
    "        # test locations\n",
    "        for (i, v) in enumerate(V):\n",
    "            plt.plot(v[0], 0, '^m', markersize=26, label='V' if i==0 else '')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_densities(p, q, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the test locations and the Gaussian bandwidth\n",
    "\n",
    "\n",
    "As in Rel-UME, the test locations $V$ and the Gaussian bandwidth can be tuned automatically by  maximizing the test power criterion of the Rel-FSSD test. The optimized test locations indicate the regions in the input domain where the better fit of $q$ can be detected with highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike Rel-UME, here we have only one sample from r since p, q \n",
    "# are density functions. No samples from p, q needed.\n",
    "# Split the data from r into training/test sets.\n",
    "datr_tr, datr_te = datr.split_tr_te(tr_proportion=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters to optimize\n",
    "# Median heuristic computed on the training set\n",
    "med = util.meddistance(datr_tr.data(), subsample=1000)\n",
    "gwidth0 = med**2\n",
    "\n",
    "# J = number of test locations\n",
    "J = 1\n",
    "Z_tr = datr_tr.data()\n",
    "V0 = util.subsample_rows(Z_tr, J, seed=seed+2)\n",
    "\n",
    "## Optimize\n",
    "opt_options = {\n",
    "    'max_iter': 100, # maximum number of gradient ascent iterations\n",
    "    'reg': 1e-3, # regularization parameter in the optimization objective\n",
    "    'tol_fun': 1e-7, # termination tolerance of the objective\n",
    "    \n",
    "    #Define a box bounds for the testing locations. \n",
    "    #extend the box defined by coordinate-wise min-max by std of each \n",
    "    #coordinate (of the aggregated data) multiplied by this number.\n",
    "    'locs_bounds_frac': 50, \n",
    "\n",
    "    'gwidth_lb': 0.1, # absolute lower bound of the Gaussian width^2\n",
    "    'gwidth_ub': 5**2, # abolute upper bound of the Gaussian width^2\n",
    "}\n",
    "V_opt, gw_opt, opt_info = mct.DC_GaussFSSD.optimize_power_criterion(p, q, datr_tr, V0, gwidth0, **opt_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimization details:')\n",
    "opt_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimized parameters to perform Rel-FSSD test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# significance level\n",
    "alpha = 0.01\n",
    "\n",
    "# An object representing a Rel-FSSD test.\n",
    "dcfssd_opt = mct.DC_GaussFSSD(p, q, gw_opt, gw_opt, V_opt, V_opt, alpha=alpha)\n",
    "\n",
    "# perform test on the test data (not the training data!)\n",
    "dcfssd_opt.perform_test(datr_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "Ztr = datr_tr.data()\n",
    "plot_densities(p, q, Ztr, V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test rejects H0. The optimized test location indicates that $q$ fits better at the right mode of $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better fit of p\n",
    "\n",
    "One can also formulate the opposite hypothesis so that the alternative hypothesis states that \"H1: p is better\". \n",
    "To optimize for test locations which indicate the better fit of $p$, one simply switches arguments $p, q$ when calling the optimization function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the first two arguments!\n",
    "V_opt2, gw_opt2, _ = mct.DC_GaussFSSD.optimize_power_criterion(q, p, datr_tr, V0, gwidth0, **opt_options)\n",
    "\n",
    "# Notice the first two arguments.\n",
    "dcfssd_opt2 = mct.DC_GaussFSSD(q, p, gw_opt2, gw_opt, V_opt2, V_opt2, alpha=alpha)\n",
    "\n",
    "# perform test on the test data (not the training data!)\n",
    "dcfssd_opt2.perform_test(datr_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_densities(p, q, Ztr, V_opt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test rejects the (reformulated) null hypothesis that $Q$ is better. The optimized location in this case indicates where $p$ fits better. \n",
    "\n",
    "We note that the behaviours of the test locations in Rel-UME and Rel-FSSD are slightly different. See the explanation of Figure 1 in our NIPS 2018 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
