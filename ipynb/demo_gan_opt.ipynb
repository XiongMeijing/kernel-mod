{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from kmod import gan_ume_opt as gt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#%config InlineBackend.figure_format = 'pdf'\n",
    "\n",
    "import kmod\n",
    "import kgof\n",
    "import kgof.goftest as gof\n",
    "# submodules\n",
    "from kmod import data, density, kernel, util\n",
    "from kmod import mctest as mct\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import utils, torch, time, os, pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class generator(nn.Module):\n",
    "    '''\n",
    "        Generative Network\n",
    "    '''\n",
    "    def __init__(self, dataset='celebA'):\n",
    "        \n",
    "        super(generator, self).__init__()\n",
    "        \n",
    "        z_size=100\n",
    "        out_size=3\n",
    "        ngf=128\n",
    "        \n",
    "        self.z_size = z_size\n",
    "        self.ngf = ngf\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # input size is z_size\n",
    "            nn.ConvTranspose2d(self.z_size, self.ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (ngf * 8) x 4 x 4\n",
    "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (ngf * 4) x 8 x 8\n",
    "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: (ngf * 2) x 16 x 16\n",
    "            nn.ConvTranspose2d(self.ngf * 2, self.ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # state size: ngf x 32 x 32\n",
    "            nn.ConvTranspose2d(self.ngf, self.out_size, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size: out_size x 64 x 64\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.ConvTranspose2d):\n",
    "                m.weight.data.normal_(0.0, 0.02)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.main(input)\n",
    "\n",
    "        return output\n",
    "    def load(self,save_dir):\n",
    "        self.load_state_dict(torch.load(save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=1,2\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../problems/celeba/img_align_celeba'\n",
    "test_img_list = []\n",
    "with open('../problems/celeba/test_list.txt') as f:\n",
    "    for line in f:\n",
    "        test_img_list.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_images(paths, size=64, resize=False):\n",
    "    img_data = []\n",
    "    for path in paths:\n",
    "        im = Image.open(path)\n",
    "        if resize:\n",
    "            im = im.resize((size, size))\n",
    "        im = np.array(im)\n",
    "        img_data.append(im)\n",
    "    return np.array(img_data)\n",
    "\n",
    "def normalize(images, mean, std):\n",
    "    \"\"\"normalize ndarray images of shape N x H x W x C\"\"\"\n",
    "    return (images - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on noise space optimization (under construction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "img_data = []\n",
    "for filename in test_img_list:\n",
    "    data_path = '{}/{}'.format(data_dir, filename)\n",
    "    im = numpy.array(Image.open(data_path))\n",
    "    img_data.append(im)\n",
    "img_data = np.array(img_data) / 255. #maybe better to normalize differently for tests\n",
    "img_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000 #number of images we want to generate\n",
    "z_dim = 100 #dimention of noise, this is fixed to 100 so don't change\n",
    "model_dir = '../problems/celeba/models/'\n",
    "gpu_id = 2\n",
    "gpu_mode = True\n",
    "gt.set_gpu_mode(gpu_mode)\n",
    "gt.set_gpu_id(gpu_id)\n",
    "gp = generator().cuda(gpu_id)\n",
    "gp.eval()\n",
    "gq = generator().cuda(gpu_id)\n",
    "gq.eval()\n",
    "gp.load('{}/GAN_G_smile_unif.pkl'.format(model_dir))\n",
    "gq.load('{}/GAN_G_nosmile_unif.pkl'.format(model_dir))\n",
    "\n",
    "sample_z_ = Variable((torch.rand((batch_size, z_dim))).view(-1, z_dim, 1, 1))\n",
    "sample_z_ = sample_z_.cuda(gpu_id)\n",
    "sample_z_ = -2. * sample_z_ + 1.\n",
    "sample_z_ = sample_z_.float()\n",
    "samples_p_show = gp(sample_z_).cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "samples_p = samples_p_show.reshape([samples_p_show.shape[0], -1])\n",
    "\n",
    "sample_z_ = Variable((torch.rand((batch_size, z_dim))).view(-1, z_dim, 1, 1))\n",
    "sample_z_ = sample_z_.cuda(gpu_id)\n",
    "sample_z_ = -2. * sample_z_ + 1.\n",
    "sample_z_ = sample_z_.float()\n",
    "samples_q_show = gq(sample_z_).cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "samples_q = samples_q_show.reshape([samples_q_show.shape[0], -1])\n",
    "\n",
    "tr_data = img_data[:15000].reshape((15000, -1))\n",
    "datap = data.Data(np.clip(samples_p, 0, 1))\n",
    "dataq = data.Data(np.clip(samples_q, 0, 1))\n",
    "datar = data.Data(tr_data[:batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "grid_size = 5\n",
    "\n",
    "for i in range(grid_size**2):\n",
    "\n",
    "    img = np.clip(samples_p_show[i], 0, 1) \n",
    "    \n",
    "    plt.subplot(grid_size, grid_size, i+1)    \n",
    "\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(grid_size**2):\n",
    "\n",
    "    img = np.clip(samples_q_show[i], 0, 1) \n",
    "    \n",
    "    plt.subplot(grid_size, grid_size, i+1)    \n",
    "\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(grid_size**2):\n",
    "    img = np.clip(img_data[i], 0, 1) \n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "J = 3\n",
    "Zp0 = Zq0 = np.random.uniform(-1, 1, (J, z_dim))\n",
    "XYZ = np.vstack((datap.data(), dataq.data(), datar.data()))\n",
    "med2 = util.meddistance(XYZ, subsample=1000)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with util.ContextTimer() as t:\n",
    "    Z_opt, gw_opt, opt_result = gt.optimize_3sample_criterion(datap, dataq, datar, gp, gq, Zp0, Zq0, gwidth0=med2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(t.secs / 60.)\n",
    "zp_opt = Variable(torch.from_numpy(Z_opt[:J]).float().view(-1, z_dim, 1, 1)).cuda(gpu_id)\n",
    "zq_opt = Variable(torch.from_numpy(Z_opt[J:]).float().view(-1, z_dim, 1, 1)).cuda(gpu_id)\n",
    "sample_p_opt = gp(zp_opt).cpu().data.numpy().transpose(0, 2, 3, 1)\n",
    "sample_q_opt = gq(zp_opt).cpu().data.numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(sample_p_opt.shape[0]):\n",
    "    img = np.clip(sample_p_opt[i], 0, 1) \n",
    "    plt.subplot(grid_size, grid_size, i+1)    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(sample_q_opt.shape[0]):\n",
    "    img = np.clip(sample_q_opt[i], 0, 1) \n",
    "    plt.subplot(grid_size, grid_size, i+1)    \n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on discrete optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using real images as samples for the three-sampel UME test, we examine test locations given by maximizing the power criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading data path\n",
    "data_dir = '../problems/celeba/img_align_celeba'\n",
    "test_img_list = []\n",
    "with open('../problems/celeba/test_list.txt') as f:\n",
    "    for line in f:\n",
    "        test_img_list.append(line.rstrip('\\n'))\n",
    "smile_img_list = []\n",
    "with open('../problems/celeba/test_smile.txt') as f:\n",
    "    for line in f:\n",
    "        smile_img_list.append(line.rstrip('\\n'))\n",
    "non_smile_img_list = [filename for filename in test_img_list \n",
    "                      if filename not in smile_img_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading image data\n",
    "paths = ['{}/{}'.format(data_dir, filename) for filename in smile_img_list]\n",
    "smile_img_data = open_images(paths, 224, resize=True)\n",
    "smile_img_data = smile_img_data / 255\n",
    "paths = ['{}/{}'.format(data_dir, filename) for filename in non_smile_img_list]\n",
    "non_smile_img_data = open_images(paths, 224, resize=True)\n",
    "non_smile_img_data = non_smile_img_data / 255\n",
    "n1 = smile_img_data.shape[0]\n",
    "n2 = non_smile_img_data.shape[0]\n",
    "tr_data = np.vstack([smile_img_data[:int(n1/2)], non_smile_img_data[:int(n2/2)]])\n",
    "te_data = np.vstack([smile_img_data[int(n1/2):], non_smile_img_data[int(n2/2):]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating training and test data\n",
    "mean = np.mean(tr_data, axis=(0, 1, 2))\n",
    "std = np.std(tr_data, axis=(0, 1, 2))\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smile_img_data_norm = normalize(smile_img_data, mean, std)\n",
    "smile_tr_data = smile_img_data_norm[:int(n1/2)]\n",
    "smile_te_data = smile_img_data_norm[int(n1/2):]\n",
    "non_smile_img_data_norm = normalize(non_smile_img_data, mean, std)\n",
    "non_smile_tr_data = non_smile_img_data_norm[:int(n2/2)]\n",
    "non_smile_te_data = non_smile_img_data_norm[int(n2/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(non_smile_img_data)\n",
    "del(smile_img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a feature extractor \n",
    "import pretrainedmodels\n",
    "model_name = 'resnet18'\n",
    "model = pretrainedmodels.__dict__[model_name]().cuda(2)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain samples for P, Q, R\n",
    "sample_size = 500\n",
    "half_n1 = int(n1 / 2)\n",
    "half_n2 = int(n2 / 2)\n",
    "\n",
    "subsample_idx_p = util.subsample_ind(half_n1, sample_size)\n",
    "datap = smile_tr_data[subsample_idx_p].reshape(sample_size, -1)\n",
    "datap = data.Data(datap)\n",
    "\n",
    "subsample_idx_q = util.subsample_ind(half_n2, sample_size)\n",
    "dataq = non_smile_tr_data[subsample_idx_q].reshape(sample_size, -1)\n",
    "dataq = data.Data(dataq)\n",
    "\n",
    "rest_tr_data = np.vstack(\n",
    "    [\n",
    "     np.delete(smile_tr_data, subsample_idx_p, axis=0),\n",
    "     np.delete(non_smile_tr_data, subsample_idx_q, axis=0)\n",
    "    ]\n",
    ")\n",
    "n = rest_tr_data.shape[0]\n",
    "datar = util.subsample_rows(rest_tr_data.reshape(n, -1),\n",
    "                            sample_size)\n",
    "datar = data.Data(datar)\n",
    "\n",
    "del(rest_tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te_data_norm = normalize(te_data, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_candidates = 500\n",
    "locs = util.subsample_rows(te_data_norm, num_candidates)\n",
    "#locs = smile_img_data_norm[-num_candidates:]\n",
    "locs = locs.reshape((locs.shape[0], -1))\n",
    "XYZ = np.vstack((datap.data(), dataq.data(), datar.data()))\n",
    "med2 = util.meddistance(XYZ, subsample=1000)**2\n",
    "J = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with util.ContextTimer() as t:\n",
    "    p_best_locs = gt.opt_greedy_3sample_criterion(datap, dataq, datar,\n",
    "                                                model.features, locs,\n",
    "                                                med2, J, maximize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with util.ContextTimer() as t:\n",
    "    q_best_locs = gt.opt_greedy_3sample_criterion(datap, dataq, datar,\n",
    "                                                model.features, locs,\n",
    "                                                med2, J, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "images = locs.reshape((num_candidates, 224, 224, 3)) * std + mean\n",
    "for i in range(len(p_best_locs)):\n",
    "    idx = p_best_locs[i]\n",
    "    img = images[idx]\n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 4\n",
    "for i in range(len(q_best_locs)):\n",
    "    idx = q_best_locs[i]\n",
    "    img = images[idx]\n",
    "    plt.subplot(grid_size, grid_size, i+1)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
